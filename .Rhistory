fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
# predict test
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)
# combine predictions
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
# confusion matrixes
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
# predict test
predict1 <- predict(fit1, newdata = vowel.test)
predict2 <- predict(fit2, newdata = vowel.test)
# combine predictions
DF_combined <- data.frame(predict1, predict2, y = vowel.test$y)
fit_combined <- train(y ~ ., data = DF_combined, method = "gam")
predict3 <- predict(fit_combined, newdata = vowel.test)
# confusion matrixes
c1 <- confusionMatrix(predict1, vowel.test$y)
c2 <- confusionMatrix(predict2, vowel.test$y)
c3 <- confusionMatrix(predict3, DF_combined$y)
c1
c2
c3
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Set the seed to 62433 and predict diagnosis with all the other variables using
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis
# ("lda") model. Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set? Is it better or worse than
# each of the individual predictions?
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf")
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
# predict test
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)
# combine predictions
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
# confusion matrixes
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
# predict test
predict1 <- predict(fit1, newdata = vowel.test)
predict2 <- predict(fit2, newdata = vowel.test)
c1 <- confusionMatrix(predict1, vowel.test$y)
c2 <- confusionMatrix(predict2, vowel.test$y)
c1
c2
library(lubridate)  # For year() function below
dat = read.csv("gaData.csv")
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
# Fit a model using the bats() function in the forecast package to the training
# time series. Then forecast this model for the remaining time points. For how
# many of the testing points is the true value within the 95% prediction
# interval bounds?
library(forecast)
library(quantmod)
# fit a model
fit <- bats(tstrain)
# check how long the test set is, so you can predict beyond trainign
h <- dim(testing)[1]
# forecast the model for remaining time points
fcast <- forecast(fit, level = 95, h = h)
# get the accuracy
accuracy(fcast, testing$visitsTumblr)
# check what percentage of times that the actual number of visitors was within
# 95% confidence interval
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
source('~/.active-rstudio-document', echo=TRUE)
install.packages("devtools")
library(devtools)
install.packages("ProjectTemplate")
pbinom(1, 20, 0.05)
pbinom(1, 18, 0.05)
pbinom(1, 1, 0.05)
dbinom(1,1,0.05)
dbinom(1, 18, 0.05)
dbinom(2, 18, 0.05)
dbinom([1:18], 18, 0.05)
dbinom(1:18, 18, 0.05)
sum(dbinom(1:18, 18, 0.05))
pbinom(18, 18, 0.05)
qbinom(1, 18, 0.05)
pbinom(1, 18, 0.95)
pbinom(1, 18, 0.85, lower.tail=FALSE)
pbinom(1, 18, 0.05, lower.tail=FALSE)
pbinom(1:18, 18, 0.05)
pbinom(17, 18, 0.95)
pbinom(1, 18, 0.05)
pbinom(1, 18, 0.05, lower.tail=FALSE)
pbinom(0, 18, 0.05)
pbinom(0, 18, 0.05, lower.tail=FALSE)
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot, s = slider(0, 2, step = 0.1))
library(rCharts)
install.packages("rCharts")
library(devtools)
install_github('ramnathv/rCharts')
data(airquality)
dTable(airquality, sPaginationType = "full_numbers")
library(ggplot2)
library(scales)
library(ggthemes)
download.file("http://s3-us-west-2.amazonaws.com/adamdata/SP500.csv",
destfile="prices.csv")
prices <- na.omit(read.csv("prices.csv",na.strings="."))
prices$DATE <- as.Date(prices$DATE)
prices$RETURN <- vector(mode="numeric",length=nrow(prices))
for (i in 2:nrow(prices)) {
prices$RETURN[i] <- 100 * log(prices$VALUE[i]/prices$VALUE[i-1])
}
extreme_vals <- prices$RETURN[prices$RETURN < quantile(prices$RETURN,
probs=c(0.025))]
extreme_vals <- sort(-1 * extreme_vals)
x <- vector(mode="numeric",length=length(extreme_vals))
x[1] <- 1
for (i in 2:length(x)) {
x[i] <- (length(x)-(i-1))/length(x)
}
tail <- data.frame(x=x,vals=extreme_vals)
View(prices)
none
========================================================
author:
date:
First Slide
========================================================
For more details on authoring R presentations click the
**Help** button on the toolbar.
- Bullet 1
- Bullet 2
- Bullet 3
Slide With Code
========================================================
```{r}
summary(cars)
```
Slide With Plot
========================================================
```{r, echo=FALSE}
plot(cars)
```
install.packages("rsconnect")
set.seed(1)
x <- as.ts(rnorm(100))
plot(x)
plot(as.ts(rnorm(100)))
plot(as.ts(rnorm(100)))
plot(as.ts(rnorm(100)))
plot(as.ts(rnorm(100)))
plot(as.ts(rnorm(100)))
plot(as.ts(rnorm(100)))
plot(as.ts(rnorm(100)))
plot(as.ts(rnorm(100)))
plot(as.ts(rnorm(100)))
methods()
methods(lm)
methods(colSums)
methods(predict)
showMethods(plot)
showMethods(mean)
showMethods(show)
getMethods(show)
findMethods(show)
getMethod(show)
showMethods(show)
gets3method(mean)
getS3method(mean)
getS3method("mean", "default")
showMethods(plot)
showMethods(show)
getMethod("show", "character")
dbinom(10, 0.2, 9)
help(dbinom)
dbinom(9, 10, 0.2)
dbinom(10, 10, 0.2)
dbinom(1, 1, 0.2)
dbinom(10, 10, 0.8)
dbinom(20, 20, 0.2)
for (i in 1:20) {}
for (i in 1:20) { dbinom(i, i, 0.8) }
for (i in 1:20) { print(dbinom(i, i, 0.8)) }
dbinom(1:20, 1:20, 0.8)
x <- dbinom(1:20, 1:20, 0.8)
y <- 1:20
plot(y, x)
plot(1:20, dbinom(1:20, 1:20, 0.9))
remove.packages("RWeka", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
remove.packages("RWekajars", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
install.packages("RWeka")
install.packages("RWeka")
library(RWeka)
library(tm)
install.packages("openNLP")
library(openNLP)
getReaders()
install.packages("hash")
install.packages("stringi")
library(stringi)
stri_install_check()
x <- 1:Inf
for (x in 1:Inf) {print(x)}
install.packages("iterators")
library(iterators)
numbers <- iter(1:Inf)
i = 0
while (i < Inf) {
print(i)
i <- i + 1
}
install.packages("Aspell", repos="http://www.omegahat.org/R")
install.packages("Aspell", repos="http://www.omegahat.org/R")
library(Aspell)
help(spellDoc)
Sys.which('grep')
.GlobalEnv
print(.GlobalEnv)
Sys.getenv()
Sys.getenv()['PATH']
Sys.getenv()['USER']
Sys.getenv()['USER'][2]
Sys.getenv()[['USER']]
SED
system('ls -alh')
do.call(system(), 'ls -alh')
do.call(system(), list('ls -alh'))
do.call(system, list('ls -alh'))
system('ls | grep none')
Sys.info()['sysname']
Sys.info()[['sysname']]
Sys.which('parallel')
Sys.which('parallel')[2]
Sys.which('parallel')[1]
Sys.which('parallel')
unname(Sys.which('parallel'))
unmame(Sys.which('hey'))
unname(Sys.which('hey'))
browser()
library(arpa)
x <- sample(seq(0, 1, by=0.001), 100000, replace=TRUE)
f <- function(x) {
return(prod(x))
}
f(x)
x <- sample(seq(0, 1, by=0.001), 1000000, replace=TRUE)
f <- function(x) {
return(prod(x))
}
f(x)
x <- sample(seq(0, 1, by=0.001), 1000000, replace=TRUE)
f <- function(x) {
return(prod(x))
}
g <- function(x) {
y <- log(x)
return(exp(sum(y)))
}
f(x)
g(x)
install.packages("microbenchmark")
library(microbenchmark)
library(ggplot2)
x <- sample(seq(0, 1, by=0.001), 1000000, replace=TRUE)
f <- function(x) {
return(prod(x))
}
g <- function(x) {
y <- log(x)
return(exp(sum(y)))
}
comp <- microbenchmark(f(x), g(x), times=1e+05)
library(microbenchmark)
library(ggplot2)
x <- sample(seq(0, 1, by=0.001), 1000000, replace=TRUE)
f <- function(x) {
return(prod(x))
}
g <- function(x) {
y <- log(x)
return(exp(sum(y)))
}
comp <- microbenchmark(f(x), g(x), times=10)
autoplot(comp)
library(microbenchmark)
library(ggplot2)
x <- sample(seq(0, 1, by=0.001), 1000000, replace=TRUE)
f <- function(x) {
return(prod(x))
}
g <- function(x) {
y <- log(x)
return(exp(sum(y)))
}
comp <- microbenchmark(f(x), g(x), times=100)
autoplot(comp)
summary(comp)
letters
sample(letters, 10, replace=TRUE)
paste0(sample(letters, 10, replace=TRUE))
paste0('x', 'a')
paste0(c('x', 'a'))
Reduce(paste0, sample(letters, 10, replace=TRUE))
Filter(function(x) x > 5, c(1,2,3,4,5,6,7,8,9,10))
Map(function(x) x*x, c(1,2,3,4,5,6,7,8,9,10))
sapply(function(x) x*x, c(1,2,3,4,5,6,7,8,9,10))
unlist(Map(function(x) x*x, c(1,2,3,4,5,6,7,8,9,10)))
Reduce('+', c(1,2,3,4,5,6,7,8,9,10))
Reduce('+', c(1,2,3,4,5,6,7,8,9,10), accumulate=TRUE)
Reduce('+', c(1,2,3,4,5,6,7,8,9,10))
df <- data.frame(x=c(1,2,3,NA),y=c(1,NA,3,4))
df
df[is.na(df)] <- 0
df
mod <- lm(mpg ~ wt, data=mtcars)
mod
summary(mod)
str(mod)
mod$df.residual
str(summary(mod))
summary(mod)$r.squared
sample(10, 3)
sample(10, 8, repeat=TRUE)
sample(10, 8)
sample(10, 8, replace=TRUE)
5 % 2
5 %% 2
6 %% 2
1:10[1:10 %% 2 == 0]
1:10 %% 2
1:10 / 2 == 0
1:10 %% 2 == 0
(1:10)[1:10 %% 2 == 0]
seq(1, 10, by=2)
seq(2, 10, by=2)
str(str)
str(?)
?str
??
)
?match
?<<-
?(<<-)
help(<<-)
help(assign)
help(get)
get('mod')
help(all.equal)
help(identical)
help(complete.cases)
help(is.na)
help(%/%)
help(signif)
help(cummax)
help(diff)
help(pmax)
help(range)
help(rle)
rle(c(1,1,1,2,3,4,4,4,5))
help(function)
function(missing)
)
help(missing)
help(on.exit)
help(invisible)
help(data.matrix)
help(sweep)
attitude
apply(attitude, 2, median)
sweep(data.matrix(attitude), 2, apply(attitude, 2, median))
help(seq_len)
seq_len(10)
seq_along(10)
help(rep_len)
help(choose)
choose(10, 2)
help(rev)
help(split)
help(expand.grid)
help(replicate)
help(chartr)
help(agrep)
help(aperm)
help(apropos)
help(q)
help(dput)
help(dget)
help(cat)
help(message)
help(warning)
help(sink)
help(format)
`+`(1, `*`(2, 3))
1 + 2 * 3
mean(x=c(1:10, NA), na.rm=TRUE)
f2 <-function(a, b) { a * 10 }
f2(10, stop('this is an error'))
install.packages("pryr")
install.packages("formatR")
install.packages('servr')
library(devtools)
has_devel()
create('~/structureR')
library(hash)
key <- 'a'
val <- list(word='bigram', logp=-7.2345)
h <- hash(keys=key, values=val)
h['a']
h[['a']]
h[['a']]$logp
rm(h, val)
val <- list(bigram=-7.2345)
h <- hash(keys=key, values=val)
h[['a']][['bigram']]
h[['a']][['dne']]
x <- 'trigram'
list(x=7)
get(x)
x
rm(h, val)
val <- list(bigram=-7.2345)
h <- hash(keys=key, values=val)
h
h[['a']] <- list(h[['a']], trigram=-9.4567)
h[['a']]
h[['a']][['bigram']]
h[['a']][['trigram']]
rm(h, val)
rm(x)
val1 <- c('bigram'=log(0.3))
val2 <- c('trigram'=log(0.7))
h <- hash(keys=key, values=c(val1, val2))
h[['a']]
h[['a']]['bigram']
h[['a']][['bigram']]
x <- 'bigram'
h[['a']][[x]]
rm(x)
h[['a']] <- c(h[['a']], 'unigram'=log(0.1))
h[['a']]
install.packages('DiagrammeR')
setwd('~/omscs/hpc/hpc-examples')
dir()
?knit2html
